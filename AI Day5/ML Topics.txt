

XXXXXXXXXXXXXXXXXX Examples on Datasets included in python libraries XXXXXXXXXXXXXXXXXX
Iris data
Digits data
Breast cancer data
Diabetes data
Sample Regression
Sample Classification
Sample images
#################### Importing the datasets from the python libraries ###########################

# Import Libraries
from sklearn.datasets import load_iris , load_digits, load_boston,load_breast_cancer,load_diabetes, make_regression, make_classification, load_sample_image



https://www.geeksforgeeks.org/what-is-dataset/

https://medium.com/@pinakdatta/unlocking-time-series-insights-deep-learning-vs-classical-methods-62bc74142f1a#:~:text=Understanding%20Time%20Series%20Data&text=These%20data%20points%20are%20typically,cyclic%20behaviour%2C%20and%20irregular%20fluctuations.


*************************** ML Model  Development Cycle ******************************

https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/well-architected-machine-learning-lifecycle.html

1- Model  =  ML Algorithm ( SVM , KNN, LR, NB, LDA, DT, RF,.., ANN, DL, ....) + Dataset 
2- Hypothesis: Equation or Mapping between the input features and the o/p of a given variable  
3- Error (Cost) Function : Finds the difference between the real and the predicted o/p of the model 
4- Model Evaluation Strategies (Cross Validation , Train Test Split)
5- Evaluation metrics (Regression metrics (MAE, MSE, RMSE , R2 ), Classification metrics (Confusion matrix  TP, TN , FP , FN ( Accuracy, Precision, Recall, ..) classification report) 

Metrics for classification:   https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions/

6- Model Deployment : Put the final model in a real working environment
7- Model Save


**************************************************** Data Preprocessing Steps *****************************************************************************************
1- Label Encoding 
2- Normalization (scaling of the features)
3- Handling Missed values and other data cleansing techniques
4- Outliers detection and handling: Detection of outliers include: IQR, LOF, KNN, ...
5- Dimensionality reduction techniques (Feature Selection or Extraction or Hybrid)
6- Models  (Regression , Classification , Clustering)

**************************************************** Label Encoding ******************************************************************************************************

Technique that is used to convert categorical columns into numerical ones so that they can be fitted by machine learning models which only take numerical data. It is an important pre-processing step in a machine-learning project.

Implementation:

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Y=le.fit_transform(Y)


**************************************************** Normalization techniques ********************************************************************************************

It is a step of Data Pre Processing that is applied to independent variables or features of data. 
It helps to normalize the data within a particular range. 
Sometimes, it also helps in speeding up the calculations in an algorithm.
Tree-based models are not dependent on scaling.
Non-tree models, very often dependent on it.
Outliers can affect certain scalers, and it is important to either remove them or choose a scalar that is robust towards them.

Techniques:

1- Standard Scaler: scales features by removing the mean and scaling to unit variance and calculated as:  z = (x - u) / s
Implementation:
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


2-Min Max Scaler: changes all features to be between 0 and 1, and calculated as:   (x-xmin)/ (xmax - xmin)
Implementation:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

3- RobustScaler: As standard scaler except that it uses median and quartiles, instead of mean and variance. 
                 Good as it ignores data points that are outliers.
Implementation
from sklearn.preprocessing.RobustScaler
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

4- Normalizer: scales vectors individually to a unit norm so that the vector has a length of one. 
               The default norm is L2, also known as the Euclidean norm. 
               The L2 norm formula is the square root of the sum of the squares of each value. 
               Although using the normalize() function results in values between 0 and 1, it’s not the same as simply scaling the values to fall between 0 and 1.

Implementation:
from sklearn.preprocessing import Normalizer
scaler = Normalizer( norm {‘l1’, ‘l2’, ‘max’}, default=’l2’)
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


https://medium.com/@reinapeh/16-data-feature-normalization-methods-using-python-with-examples-part-1-of-3-26578b2b8ba6

https://python-data-science.readthedocs.io/en/latest/normalisation.html

********************************************************************************************************************************************************************************
Base Classifiers

1-Logistic Regression:  

2-SGD: Stochastic Gradient Descent classifier:  can be used to implement logistic regression (loss = log_loss) ,  support vector machines (loss='hinge'), perceptron , .. 

3-SVC: Support Vector Machines Classsifier : can be used with different kernel types including linear, poly, rbf, sigmoid

4-KNeighbors Classifier : Kth Nearest Neighbour classifier 

5-Decision Tree Classifier

6-Linear Discremnant Analysis

7-Naive Bayes

https://scikit-learn.org/
http://rinterested.github.io/statistics/logistic_regression.html
http://rasbt.github.io/mlxtend/user_guide/classifier/LogisticRegression/
https://scikit-learn.org/stable/modules/sgd.html
https://www.topcoder.com/thrive/articles/support-vector-machine-a-machine-learning-algorithm
https://medium.com/low-code-for-advanced-data-science/support-vector-machines-svm-an-intuitive-explanation-b084d6238106
https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/
https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/
https://www.ibm.com/topics/decision-trees
https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/
https://www.analyticsvidhya.com/blog/2021/08/a-brief-introduction-to-linear-discriminant-analysis/
https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/

Ensemble Classifiers:  combines several base models to produce the final optimum solution. 
https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/

1- Voting Classifier
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html
https://www.naukri.com/code360/library/the-voting-classifier

2- Bagging classifiers (Random forest)
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#baggingclassifier

3- Boosting classifiers  (Ada-boost, Gradient Boost, XGBoost)
https://www.simplilearn.com/ensemble-learning-article
https://www.ibm.com/topics/ensemble-learning#:~:text=Ensemble%20learning%20is%20a%20machine,1

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Model Evaluation Strategies
1- Train Test Split : Split the dataset into train and test sets only.
2- Cross Validation: Split the data set into k folds and returns a mean performance metrics (k folds , stratified k folds, Repeated , Repeated stratified , loocv cross validation)

rom sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

kfold = KFold(n_splits=5, random_state=45, shuffle=True)
model = LogisticRegression(solver='liblinear',random_state=45)
results = cross_val_score(model, X, Y, cv=kfold)


https://scikit-learn.org/stable/modules/cross_validation.html
https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators

**************************************************** Hyperparameters Optimization Techniques *****************************************************************************

Hyperparameters are essential parameters that must be determined before developing the model 
Examples on the hyperparameters : K neighbours (KNN classifier), Regularization type (L1, L2, Elastic net for Logistic Regression), Kernel Type (rbf, linear, poly, sigmoid for support vector machines)


Examples on the logistic regression hyperparameters and parameters

Regularization type (Penalty)

penalty{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’

C:  default=1.0 : Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.

solver {‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default=’lbfgs’.  Algorithm to use in the optimization problem. Default is ‘lbfgs’.

max_iterint: default=100.   Maximum number of iterations taken for the solvers to converge.

l1_ratio:  default=None. The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. 


https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac
https://medium.com/@ompramod9921/model-parameters-and-hyperparameters-in-machine-learning-502799f982d7#:~:text=A%20parameter%20is%20a%20variable,before%20the%20training%20process%20begins.
https://www.geeksforgeeks.org/difference-between-model-parameters-vs-hyperparameters/

Techniques to optimize the hyperparameters are: These techniques are used along with the cross validation 
1- Grid search
2- Random search 
3- Bayesian Optimization
4- Naturally inspired algorithms (genetic Algorithm ,...)

https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators

##################################################################################################################

